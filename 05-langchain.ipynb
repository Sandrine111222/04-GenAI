{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7061d513",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangChain is the most popular open source library to interact with multiple LLMs and RAG systems in a unified framework. You may love it, you may hate it, but you have to know how to use it!\n",
    "\n",
    "Let us try to make a system similar to the previous notebook in a few easy steps. The first thing to do:\n",
    "\n",
    "`pip install -qU \"langchain[google-genai]\"`\n",
    "\n",
    "⚠️LangChain changes all the time. This notebook was generated using the 0.3.27 version.\n",
    "\n",
    "How to call the model using LangChain linguo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash-lite\", model_provider=\"google_genai\")\n",
    "\n",
    "model.invoke(\"I like frogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cd63b",
   "metadata": {},
   "source": [
    "As you can see, it is easy to call the model from here. The output also has many different and handy metadata that comes with it. Langchain even goes to look for the API key on its own in the files around. As long as you call it GOOGLE_API_KEY, it'll find it.\n",
    "\n",
    "Let's check what the \"model\" variable is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c91f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24658ee7",
   "metadata": {},
   "source": [
    "As you can see, langchain has a special object for the google generative models. In fact it does for every major provider, and you can interact with all of them using the same langchain functions, this is one of the reasons it makes for such a useful package.\n",
    "\n",
    "For example if you want to simulate a chat with a model, you may do so in lanchain using LLMChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81152bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1 – prompt that leaves a slot for prior turns\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),    # <── chat lives here\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 2 – memory that feeds the placeholder\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                  return_messages=True)\n",
    "chat  = LLMChain(llm=model, prompt=prompt, memory=memory)\n",
    "\n",
    "# 3 – minimal interactive loop\n",
    "while True:\n",
    "    user = input(\"You: \")\n",
    "    if user.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "    print(\"You:\", user)\n",
    "    print(\"AI:\", chat.predict(input=user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b240c",
   "metadata": {},
   "source": [
    "It is also much easier to perform RAG using langchain. Take the example from our previous notebook. Using langchain to build this rag system is a piece of cake. It may look like some alien language at first, especially with all the objects and classes. But rest assured that langchain allows for building in all directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- 1) Load & chunk your document ---\n",
    "path = \"data\\\\becode_rules.txt\"\n",
    "docs = TextLoader(path, encoding=\"utf-8\").load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=80)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# --- 2) Build a tiny local index (FAISS) with Google embeddings ---\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vs = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# --- 3) Minimal RAG prompt & chain ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You answer using ONLY the context. If unsure, say you don't know.\\n\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {  # fetch context from retriever, pass the raw question through\n",
    "      \"context\": retriever | format_docs,\n",
    "      \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- 4) Ask stuff ---\n",
    "print(rag_chain.invoke(\"When are the Moodle check-ins?\"))\n",
    "print(rag_chain.invoke(\"How long can internships last in Brussels?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf08c8b",
   "metadata": {},
   "source": [
    "If you want to dive deeper on LangChain. It is an immensely popular module, there are plenty of online resources about it! You may start on [Datacamp](https://app.datacamp.com/learn/courses/developing-llm-applications-with-langchain). Good luck! And have Fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
